# My Mind

## ğŸ¤– LLM-Powered PDF & Notes ETL Pipeline (Python)

This project provides a **modular ETL pipeline** in Python that transforms raw PDF files â€” including scanned images â€” into structured Markdown files. These can be used as **contextual data sources for LLM-based systems** (e.g. retrieval-augmented generation, semantic search, assistants, or chatbots).

The system is optimized for **personal knowledge management**, but can be easily scaled to support **enterprise document processing**.


## ğŸ¯ Key Features

- âœ… Extracts text from both digital and scanned PDFs using **EasyOCR**
- âœ… Converts pages to images for OCR using **pdf2image** or **PyMuPDF**
- âœ… Cleans and normalizes text before saving it to `.md` files
- âœ… Supports recursive folder traversal
- âœ… Uses **LangChain + OpenAI** to embed content (optional)
- âœ… Modular architecture for ETL: `extract/`, `transform/`, `load/`, `utils/`, `config/`
- âœ… Automatic temporary file cleanup
- âœ… Logs errors and processes robustly, PDF-by-PDF

---

## ğŸ—ï¸ Project Structure

```

project\_root/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py           # Configuration parameters (paths, OCR langs, logging)
â”œâ”€â”€ extract/
â”‚   â”œâ”€â”€ pdf\_to\_image.py       # Convert PDF pages to images
â”‚   â””â”€â”€ ocr.py                # Extract text from images using EasyOCR
â”œâ”€â”€ transform/
â”‚   â””â”€â”€ text\_cleaner.py       # Optional text normalization and cleanup
â”œâ”€â”€ load/
â”‚   â””â”€â”€ markdown\_writer.py    # Generate structured Markdown output
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ file\_utils.py         # Directory traversal, file name sanitization, etc.
â”‚   â””â”€â”€ logging\_setup.py      # Centralized logging configuration
â”œâ”€â”€ main.py                   # Orchestrates the full ETL flow
â”œâ”€â”€ requirements.txt          # Python dependencies
â””â”€â”€ output/                   # Markdown files generated from PDFs

````

---

## âš™ï¸ Setup Instructions

### 1. Clone and install dependencies

```bash
git clone https://github.com/your-org/pdf-etl-pipeline.git
cd pdf-etl-pipeline
pip install -r requirements.txt
````

### 2. Configure your `.env` (for OpenAI, if using embeddings)

Create a `.env` file in the project root:

```
OPENAI_API_KEY=sk-...
```

### 3. Configure pipeline settings

Edit `config/settings.py` to adjust:

```python
input_dir = "input/"
output_dir = "output/"
ocr_languages = ['en', 'pt']  # EasyOCR supported languages
chunk_size = 1000             # For LangChain chunking (optional)
chunk_overlap = 100
```

---

## ğŸ§  How It Works

### ğŸ“¥ Extraction Phase (`extract/`)

* `pdf_to_image.py`: Converts every PDF page into a high-res `.png` image
* `ocr.py`: Uses EasyOCR to read text from each image

### ğŸ§¹ Transformation Phase (`transform/`)

* `text_cleaner.py`: Optional cleaning (e.g., remove artifacts, normalize unicode)

### ğŸ“¤ Load Phase (`load/`)

* `markdown_writer.py`: Writes one `.md` file per PDF with sections per page
* Output example:

  ```markdown
  ---
  pdf: my_report.pdf
  pages: 5
  ---
  ## Page 1
  (text here)
  ## Page 2
  (text here)
  ```

---

## ğŸ§ª Running the Pipeline

Use the following command to process all PDFs:

```bash
python main.py
```

Each PDF will be:

1. Converted to images
2. Processed with OCR
3. Converted into clean Markdown
4. Saved in the `/output` folder

All temporary files are cleaned up automatically.

---

## ğŸ“Œ Error Handling & Logging

* All processing steps are wrapped in `try/except` blocks.
* Errors are logged using Python's `logging` module (`pipeline.log`).
* Faulty PDFs are skipped, the pipeline continues.

---

## ğŸ”— Optional: Embedding with LangChain + OpenAI

To enable document chunking and embeddings (e.g. for vector storage):

```python
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS

embeddings = OpenAIEmbeddings()
db = FAISS.from_texts(text_chunks, embeddings)
```

This allows the Markdown chunks to be embedded and queried in a RAG pipeline.

---

## ğŸ§  Use Cases

* ğŸ§˜ Personal LLM-powered assistants
* ğŸ“‚ Semantic search over your notes
* ğŸ“‘ Legal or HR document processors
* ğŸ¤– Enterprise knowledge bots
* ğŸ“ Structured summarization from PDFs

---

## ğŸ›£ï¸ Roadmap

* [ ] Add Whisper transcription support for audio PDFs
* [ ] Add front-end drag-and-drop web interface
* [ ] Integrate with vector DBs (Weaviate, Pinecone, Qdrant)
* [ ] Export to other formats (JSON, CSV, Obsidian vault)

---

## ğŸ“„ License

MIT License. Feel free to use, fork, and adapt.

---

## ğŸ™‹â€â™‚ï¸ Contributing

Pull requests are welcome. For major changes, open an issue first to discuss the direction.

---

## ğŸ‘‹ Contact

Made with â¤ï¸ by \[Your Name]
ğŸ“¬ Email: [your@email.com](mailto:your@email.com)
ğŸŒ Linkedin: [https://github.com/yourusername](https://github.com/yourusername)

```

